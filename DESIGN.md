Within our HUD-Bud Directory, we have the following folders/files:
# static/
Contains our styles.css file, in which our initialize/edit profile page is styled and our plan tables are structured with alternating colored rows for ease of the user’s viewing.
The logo for our site, a veritaffle, is stored as a png in this folder as well. Lines 0-49 are borrowed from the style code from the “CS50 Finance” problem set.
# templates/
Our various HTML pages reside here, with the generic layout.html being the template applied to most web pages to showcase the top bar of the webpage with the different tabs, as well as
the mainstream title of our site and the logo. There is a register page to allow new users to create accounts with HUD-Bud, a login page to allow existing users to use the program, a
profile initialization page for new users to submit a form entailing daily caloric intake and dietary restrictions/allergens, an edit profile page that allows for manipulations of the
previously stated inputs, and a plan.html  page for displaying the generated plans based off the data that was web scraped from the HUDS website.
# requirments.txt
This text file prescribes the packages upon which this app will depend on. This is borrowed from “CS50 Finance”.
# helpers.py
This file contains some helper functions to be used in the main “application.py” file. This is borrowed from “CS50 Finance”.
# huds.db
This is the centralized database in which information about the users of the program as well as the daily menu listing from the web scrape algorithm are stored.
- users: stores user id for logging the session_id in Python, and the hash of the user’s password
- default_settings: stores the user’s caloric intake in the profile, and whether a plan has been generated yet for that user
- allergens: stores allergen id and associated name for allergen, with the inclusion of ‘no allergens’ and ‘vegetarian’ and ‘vegan’ as other potential flags to add to food items during
the process of web scraping to characterize the food items more specifically
- user_allergens: stores user id and allergen id, in which the allergens noted by the user as restrictions in the initialize/edit profile page are stored; this allows for eventually filtering
out the foods that the user isn’t able to eat when generating the meal plans
- foods: stores food id, name of food, associated calories, all nutrients web scraped from the HUDs website, and a balanced_nutrition_index that is calculated in application.py to determine
the foods that contain the most valuable nutrients according to what is rarest in terms of nutrients presence on a given day
- food_allergens: stores food id and allergen id, thereby linking the associated allergens with specific foods, as determined through the web scraping process
# application.py
This is the main file from which the flask and python code for the program is run.
Lines 15-35 include code pertaining to configuring the flask session, which is also borrowed from “CS50 Finance”.

# ROUTES
There are several routes specified, such as register, login, logout, initialize profile, edit profile, and index, and the functionality behind these are detailed in the documentation
README.md. The use of GET and POST requests support these different routes for rendering pages while passing in parameters for use with jinja and HTML, as well as for submitting forms in
HTML to use data on the back end and store/retrieve data from huds.db.

# WEB SCRAPING
The call to web-scrape is made in /index with scrape_day() on line 356. NOTE: After this has been done for the day, please comment out this line to prevent running this scrape algorithm
multiple times each day, as it isn’t necessary to do so.

To start, in scrape_day() the URLs to be scraped are dynamically generated by taking the known stem of the URL and appending the current date (with Python datetime library) and meal code
(Monday-Saturday: 0 = breakfast, 1 = lunch, dinner = 2; Sunday: 0 = lunch, 1 = dinner). The URL for each meal and corresponding meal code are passed into the next, more narrowly-focused,
function scrape_meal(url, meal). Regardless of the day, two calls to scrape_meal() are made, and a third call is made only if the current day of the week (accessed through Python datetime
library) is Monday-Saturday.

In scrape_meal, we instantiate a BeautifulSoup object, which is an instance of a Python class used to extract information from HTML, parse HTML data, and web scrape. We spent a lot of time
understanding the DOM and structure of the HUDs website to know which HTML tags to focus on and how to go about extracting and massaging the data in the form we desired in more complex
situations. In this function, we created a list of lists called URL_list, containing the associated link to web scrape for each food, whether that food is an entree, whether that food is
vegetarian, and whether that food is vegan. To collect this data, we went through the table of foods on each meal webpage, noting through a boolean when we were starting on a list of entrees
or not so that this information could be stored with the specific food items nested beneath. By accessing each food label through specific classes in the HTML file, we noted whether it was
flagged as vegan or vegetarian through additional parameters (lines 95-119). We then iterated through this list, calling scrape_item(URL_list) on each list containing the food’s URL to web
scrape, entree status, vegetarian status, and vegan status.

Inside scrape_item(URL_list), another Beautiful Soup object is instantiated and we undergo a similar process of finding the necessary HTML tags and portions of the code to extract caloric
intake, allergens, and nutrient information from. All of these fields about a food item are stored in a dictionary (line 75), with the last key:value pair of nutrients being a dictionary of
nutrient_name: daily_value pairs as well. This data was then returned in that for loop in scrape_meal in line 126, where the data is then stored in the SQL database foods table through a
series of queries, depending on the day of the week and whether the entry for that particular food in the table already exists (handled duplicates) (lines 128-161). The food’s associated
allergens are also stored in the food_allergens table (lines 164-187).

# GENERATING PLAN
We calculated an index for each food item as a linear combination of the frequency of each nutrient (balanced_nutrition_index(nutrient_weights, food)). The weight applied is the complement
of the frequency of a particular nutrient / frequency of all nutrients together, so that the rarest nutrient on a given day has the highest weight to be prioritized the most
(calculate_nutrient_weights(foods)). The foods by meal are then ordered by index, so that highest indexed foods are valued the most in terms of probability of being put into the meal plan.
For Monday-Saturday, breakfast gets 20% of calories, and 40% each for lunch and dinner; on Sunday, lunch and dinner each take 50% of daily caloric intake. The foods that are eligible for
the user to eat are then selected through SQL queries (lines 277-312). For each meal, at least one entree is part of the plan, with the entree being the highest-indexed one according to the
index algorithm described earlier. The remaining foods inputted are also in order of index to maintain a balanced nutritional diet according to the index measurement, and foods are inputted
into the plan up until the maximum caloric intake has been reached or until no more foods can be added sufficiently to stay under the limit. These plans are then passed into plan.html via
render template in the index path to be displayed to the user.
